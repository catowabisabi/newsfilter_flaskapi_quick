import os
import requests
import time
import json
from dotenv import load_dotenv
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from seleniumwire import webdriver
from selenium.webdriver.chrome.options import Options
import time
from bs4 import BeautifulSoup
from lxml import etree
import tempfile
from login import NewsfilterLogin




class NewsfilterApi:
    def __init__(self):
        load_dotenv(override=True)
        self.access_token = os.getenv("ACCESS_TOKEN")
        self.refresh_token = os.getenv("REFRESH_TOKEN")
        self.search_api_url = os.getenv("SEARCH_API_URL")
        self.token_refresh_url = os.getenv("TOKEN_REFRESH_URL")
        self.token_file = ".env"  # å‡è¨­ä½ ç”¨ .env åšå„²å­˜
        self.last_refresh_time = time.time()
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.access_token}",
            "Origin": "https://newsfilter.io",
            "Referer": "https://newsfilter.io/",
            "User-Agent": "Mozilla/5.0 ..."
        }

    def login(self):
        login_client = NewsfilterLogin()
        access_token, refresh_token = login_client.login()
        
        if access_token and refresh_token:
            self.access_token = access_token
            self.refresh_token = refresh_token
            
            # æ›´æ–° headers
            self.headers["Authorization"] = f"Bearer {self.access_token}"
            
            # æ›´æ–° .env æª”æ¡ˆ
            self._update_env_token("ACCESS_TOKEN", self.access_token)
            self._update_env_token("REFRESH_TOKEN", self.refresh_token)
            
            self.last_refresh_time = time.time()
            print("ğŸ” Login successful.")
            return True
        else:
            raise Exception("âŒ Login failed: Could not obtain tokens")

    def get_access_token(self):
        # å¦‚æœ access token å¤ªèˆŠï¼ˆå‡è¨­ 12 å°æ™‚ï¼‰ï¼Œå°± refresh
        if time.time() - self.last_refresh_time > 12 * 3600:
            print("âš ï¸ Access token å¯èƒ½éæœŸï¼Œå˜—è©¦ refresh...")
            self.login()
        return self.access_token

    def get_refresh_token(self):
        return self.refresh_token

    def _update_env_token(self, key, value):
        try:
            lines = []
            updated = False
            
            # è®€å–ç¾æœ‰çš„ .env æ–‡ä»¶
            if os.path.exists(self.token_file):
                with open(self.token_file, "r") as f:
                    for line in f:
                        if line.startswith(f"{key}="):
                            lines.append(f"{key}={value}\n")
                            updated = True
                        else:
                            lines.append(line)
            
            # å¦‚æœ key ä¸å­˜åœ¨ï¼Œæ·»åŠ æ–°çš„
            if not updated:
                lines.append(f"{key}={value}\n")
            
            # å¯«å…¥æ›´æ–°å¾Œçš„å…§å®¹
            with open(self.token_file, "w") as f:
                f.writelines(lines)
            
            print(f"âœ… Updated {key} in {self.token_file}")
        except Exception as e:
            print(f"âŒ Failed to update {self.token_file}: {str(e)}")
    
    def get_articles_by_symbol(self, symbol: str):
        print(f"ğŸ” æ­£åœ¨ç²å– {symbol} ç›¸é—œæ–‡ç« ...")
        def call_api():
            headers = self.headers
            payload ={"type":"filterArticles","isPublic":False,"queryString":f"title:\"{symbol}\" OR description:\"{symbol}\" OR symbols:\"{symbol}\"","from":0,"size":50}

            return requests.post(self.search_api_url, json=payload, headers=headers)

        self.response = call_api()

        # 1. å¦‚æœ status code æ˜¯ 401ï¼Œå…ˆ login å† retry
        if self.response.status_code == 401:
            print("âš ï¸ Token expired. Refreshing...")
            self.login()
            self.response = call_api()

        # 2. å¦‚æœæ˜¯ 200ï¼Œä½† articles æ˜¯ç©ºçš„ï¼Œéƒ½è©¦ login å† retry
        if self.response.status_code == 200:
            self.articles_data = self.response.json()   
            if "articles" in self.articles_data and len(self.articles_data["articles"]) == 0:
                print("âš ï¸ 200 OK ä½†ç„¡å…§å®¹ï¼Œå˜—è©¦ refresh token å†æ’ˆ...")
                self.login()
                self.response = call_api()
                self.articles_data = self.response.json()
            print(f"ğŸ” {symbol} æ–‡ç« æ•¸é‡: {len(self.articles_data['articles'])}")
            return self.articles_data

        # 3. å…¶ä»–éŒ¯èª¤
        raise Exception(f"âŒ Failed to fetch articles: {self.response.status_code} {self.response.text}")
    
    def print_articles(self):
    
        if self.articles_data["articles"] == []:
            print("âŒ No articles data available for today and yesterday")
            return
        for i, article in enumerate(self.articles_data.get("articles", []), 1):
            print(f"\nğŸ“° Article {i}")
            print(f"ğŸ“… Published At : {article.get('publishedAt')}")
            print(f"ğŸ§¾ Title        : {article.get('title')}")
            print(f"ğŸ“ Description  : {article.get('description')}")
            print(f"ğŸ“° Source       : {article.get('source', {}).get('name')}")
            #print(f"ğŸ“ˆ Symbols      : {', '.join(article.get('symbols', []))}")
            #print(f"ğŸ·ï¸  Industries   : {', '.join(article.get('industries', [])) if 'industries' in article else 'N/A'}")
            #print(f"ğŸ­ Sectors      : {', '.join(article.get('sectors', [])) if 'sectors' in article else 'N/A'}")
            print(f"ğŸ–¼ï¸  Image URL    : {article.get('imageUrl', 'N/A')}")
            print(f"ğŸ”— Source URL   : {article.get('sourceUrl', 'N/A')}")
            print(f"ğŸ“ Newsfilter   : {article.get('url')}")
            print(f"ğŸ“ HTML Content : {article.get('html_content')[:100]}...")

    def filter_by_date_range(self, articles, start_date, end_date):
        filtered = []
        for article in articles:
            published_at = article.get("publishedAt")
            if published_at:
                try:
                    # å°‡ "Z" æ”¹æˆ "+00:00"ï¼Œå°‡ "+0000" æ”¹æˆ "+00:00"
                    ts = published_at.replace("Z", "+00:00").replace("+0000", "+00:00")
                    pub_date = datetime.fromisoformat(ts).date()
                    if start_date <= pub_date <= end_date:
                        filtered.append(article)
                except Exception as e:
                    print(f"âš ï¸ æ—¥æœŸè§£æéŒ¯èª¤: {published_at}")
        return filtered

    def _download_html(self,url):
        headers = self.headers
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()  # å¦‚æœ status code å””ä¿‚ 200 å°±æœƒå ±éŒ¯
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"ä¸‹è¼‰å¤±æ•—ï¼š{e}")

    def get_today_and_yesterday_articles(self, symbol: str):
        all_articles_data = self.get_articles_by_symbol(symbol)
        all_articles = all_articles_data.get("articles", [])

        est = ZoneInfo("America/New_York")
        today = datetime.now(est).date()
        yesterday = today - timedelta(days=1)

        filtered_articles = self.filter_by_date_range(all_articles, yesterday, today)
        self.articles_data = {"articles": filtered_articles}
        print(f"ğŸ” ä»Šå¤©èˆ‡æ˜¨å¤©çš„æ–‡ç« æ•¸é‡: {len(filtered_articles)}")
        self.get_html_content()
        return self.articles_data


    def get_html_content(self):

        def download_rendered_html(url):
            # è¦åŠ å˜…è‡ªå®š header
            headers = self.headers

            # Selenium Wire çš„ header è¨­å®š
            seleniumwire_options = {
                'custom_headers': headers
            }

            options = Options()
            options.add_argument('--headless')  # ç„¡é ­
            options.add_argument('--disable-gpu')
            options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                     "(KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36")
            
            tmp_profile = tempfile.mkdtemp()
            options.add_argument(f'--user-data-dir={tmp_profile}')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument('--disable-blink-features=AutomationControlled')


            driver = webdriver.Chrome(options=options, seleniumwire_options=seleniumwire_options)
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            driver.get(url)
            time.sleep(5)  # ç­‰ JS è¼‰å®Œ

            rendered_html = driver.page_source
                   

            
            # BeautifulSoup è§£æç”¨æ–¼å…¶ä»–æ–¹æ³•
            soup = BeautifulSoup(rendered_html, "html.parser")
            
            # æ–¹æ³•2: å°‹æ‰¾ç‰¹å®š class çš„ div å’Œå…¶ä¸­çš„æ®µè½
            article_div = soup.find("div", class_="col-lg-10 col-lg-offset-1")
            if article_div:
                paragraphs = article_div.find_all("p")
                article_text = "\n\n".join([p.get_text(strip=True) for p in paragraphs])
                if article_text.strip():
                    driver.quit()
                    return article_text
            
                        # æ–¹æ³•1: ä½¿ç”¨ XPath æå–å…§å®¹
            try:
                html_tree = etree.HTML(rendered_html)
                content_elements = html_tree.xpath('//*[@id="root"]/div[2]/div[2]/div/div[1]/div')
                if content_elements:
                    article_text = content_elements[0].text_content().strip()
                    if article_text:
                        driver.quit()
                        return article_text
            except Exception as e:
                print(f"XPath extraction failed: {e}")

            # æ–¹æ³•3: å°‹æ‰¾ article æ¨™ç±¤
            article_tag = soup.find("article")
            if article_tag:
                paragraphs = article_tag.find_all("p")
                article_text = "\n\n".join([p.get_text(strip=True) for p in paragraphs])
                if article_text.strip():
                    driver.quit()
                    return article_text

            # æ–¹æ³•4: å°‹æ‰¾å¸¶æœ‰ article æˆ– content ç›¸é—œ class çš„ div
            content_divs = soup.find_all("div", class_=lambda x: x and ('article' in x.lower() or 'content' in x.lower()))
            for div in content_divs:
                paragraphs = div.find_all("p")
                article_text = "\n\n".join([p.get_text(strip=True) for p in paragraphs])
                if article_text.strip():
                    driver.quit()
                    return article_text

            # æ–¹æ³•5: å°‹æ‰¾æœ€é•·çš„æ–‡å­—å…§å®¹å€å¡Š
            all_divs = soup.find_all("div")
            max_text_length = 0
            max_text_content = ""
            for div in all_divs:
                text_content = div.get_text(strip=True)
                if len(text_content) > max_text_length:
                    max_text_length = len(text_content)
                    max_text_content = text_content

            driver.quit()
            return max_text_content if max_text_length > 100 else "No content found"


        for article in self.articles_data["articles"]:
            html_content = download_rendered_html(article["url"])
            article["html_content"] = html_content

        return self.articles_data
        








if __name__ == "__main__":
    api = NewsfilterApi()
    token = api.get_access_token()
    print(f"ğŸ” ACCESS_TOKEN: {token[:30]} ... ")
    articles_data = api.get_today_and_yesterday_articles("ALG")


    api.print_articles()


    



